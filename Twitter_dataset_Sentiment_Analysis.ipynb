!pip install neattext

Collecting neattext
  Downloading neattext-0.1.3-py3-none-any.whl (114 kB)
2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.7/114.7 kB 3.6 MB/s eta 0:00:00
?25hInstalling collected packages: neattext
Successfully installed neattext-0.1.3

# Import all the necessary libraries
import pandas as pd
import numpy as np
import re
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords
import neattext.functions as nt
from textblob import TextBlob

[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package wordnet to /root/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!

# Importing the dataset
DATASET_COLUMNS=['target','ids','date','flag','user','text']
DATASET_ENCODING = "ISO-8859-1"
twitter_DataFrame = pd.read_csv('/content/Twitter_Dataset.csv', encoding=DATASET_ENCODING, names=DATASET_COLUMNS)
twitter_DataFrame.sample(5)

	target 	ids 	date 	flag 	user 	text
604345 	0 	2221918346 	Thu Jun 18 06:12:46 PDT 2009 	NO_QUERY 	crosler 	Cats and dogs! Global cooling has hit NEPA. I ...
896650 	4 	1693089409 	Sun May 03 21:32:09 PDT 2009 	NO_QUERY 	FLYWORLD1 	This chicken is good as hell
955398 	4 	1825027549 	Sun May 17 04:01:13 PDT 2009 	NO_QUERY 	Jaaaaamm 	i've just joined twitter and it's great!
519221 	0 	2191738688 	Tue Jun 16 05:52:49 PDT 2009 	NO_QUERY 	emmaarrr 	@TimPott I don't wannnaaaaa do more
305332 	0 	1999966070 	Mon Jun 01 21:09:08 PDT 2009 	NO_QUERY 	internalquest 	@heavenlee732 Sorry

sample_size = 80000
sampled_data = twitter_DataFrame.sample(n=sample_size, random_state=42)  # You can use any random_state value for reproducibility

sampled_data.shape

(80000, 6)

sampled_data.info()

<class 'pandas.core.frame.DataFrame'>
Int64Index: 80000 entries, 462211 to 122510
Data columns (total 6 columns):
 #   Column  Non-Null Count  Dtype 
---  ------  --------------  ----- 
 0   target  80000 non-null  int64 
 1   ids     80000 non-null  int64 
 2   date    80000 non-null  object
 3   flag    80000 non-null  object
 4   user    80000 non-null  object
 5   text    80000 non-null  object
dtypes: int64(2), object(4)
memory usage: 4.3+ MB

sampled_data['text'].isnull().any()

False

# Function to clean the dataset using regular expression
def clean_tweet(tweet):
    '''
    Function to clean the tweets by removal of  links, special characters,
    and stopwords using regular expressions and NLTK.
    '''
    # remove URLs, RTs, and mentions
    tweet = re.sub(r'http\S+', '', tweet)
    tweet = re.sub(r'RT[\s]+', '', tweet)
    tweet = re.sub(r'@[A-Za-z0-9_]+', '', tweet)

    # remove non-alphanumeric characters
    tweet = re.sub(r'\W+', ' ', tweet)

    # remove stopwords
    stop_words = set(stopwords.words('english'))
    additional_stopwords = {
        'rt', 'amp', 'via', 'ht', '...', '@username', 'http', 'https', 'www',
        'lol', 'omg', 'wtf', 'imho', 'fyi', 'tl', 'dr', 'bff', 'dm', 'mt', 'ht',
        'ff', 'ftw', 'brb', 'tbh', 'irl', 'jk', 'nvm', 'smh', 'tbh', 'tk', 'ymmv',
        'yolo', 'rofl', 'lmao', 'smh', 'af', 'fomo', 'hmu', 'icymi', 'imo', 'ily',
        'mfw', 'mrw', 'nsfw', 'omw', 'tfw', 'tbt', 'tmi', 'ttys', 'ykwim', 'yw',
        'irl', 'rtfm', 'gbtw', 'ygm', 'yolo', 'yoyow', 'ywia', 'ywsyl', 'ywsyyg'
    }
    stop_words |= additional_stopwords
    words = nltk.word_tokenize(tweet)
    words = [word.lower() for word in words if word.lower() not in stop_words]

    # join the words back into a string
    cleaned_tweet = ' '.join(words)

    return cleaned_tweet

sampled_data['clean_tweet'] = sampled_data['text'].apply(clean_tweet)

sampled_data['clean_tweet'].head(20)

462211        would go bad eventually hey make fun b c rain
906600                      good morning hoping good one ya
953710                        makeup stuffs gon na good day
777548          crying sleep really really really wanted go
644707                          getting hungry 10 50am fair
78646         says one thing really really miss booby hatch
892632    planted peas basil cat nip parsley fire escape...
491665    nice massage rest day going good excited santa...
76563     cat pased away today thats really sad day toda...
17327                               wish seen sf show night
339439             still moving imap mails exchange servers
440628                               play sit lonely office
730496                        sigh first soccer loss season
119766                                      mark happy face
824901       well ur drinking water surprise ur drunk silly
971266                msn aim let chat send screenames msns
283597    ive got ta go work everyone hit always bruise ...
417207                                        wish done yet
405795                 would honestly tried take solo cards
330997    god start thinking work horse racing novel dec...
Name: clean_tweet, dtype: object

PreProcessed_Data = sampled_data[['ids', 'clean_tweet']]

PreProcessed_Data.head(20)

	ids 	clean_tweet
462211 	2174633709 	would go bad eventually hey make fun b c rain
906600 	1695475403 	good morning hoping good one ya
953710 	1824695185 	makeup stuffs gon na good day
777548 	2322447234 	crying sleep really really really wanted go
644707 	2236096768 	getting hungry 10 50am fair
78646 	1751461265 	says one thing really really miss booby hatch
892632 	1691270862 	planted peas basil cat nip parsley fire escape...
491665 	2183909614 	nice massage rest day going good excited santa...
76563 	1695724319 	cat pased away today thats really sad day toda...
17327 	1556241519 	wish seen sf show night
339439 	2014642275 	still moving imap mails exchange servers
440628 	2066787568 	play sit lonely office
730496 	2263587261 	sigh first soccer loss season
119766 	1831140495 	mark happy face
824901 	1556358703 	well ur drinking water surprise ur drunk silly
971266 	1831132895 	msn aim let chat send screenames msns
283597 	1992945647 	ive got ta go work everyone hit always bruise ...
417207 	2061481362 	wish done yet
405795 	2058775667 	would honestly tried take solo cards
330997 	2012507589 	god start thinking work horse racing novel dec...

PreProcessed_Data['clean_tweet'].isnull().any()

False

PreProcessed_Data.to_csv('preProcessedTwitterData.csv', index=False)

PreProcessed_Data.head()

	ids 	clean_tweet
462211 	2174633709 	would go bad eventually hey make fun b c rain
906600 	1695475403 	good morning hoping good one ya
953710 	1824695185 	makeup stuffs gon na good day
777548 	2322447234 	crying sleep really really really wanted go
644707 	2236096768 	getting hungry 10 50am fair

# Check Sentiment
# Function to check sentiment of tweets using TextBlob
def sentimentCheck(tweet):
    text = TextBlob(tweet)
    sentiment = text.sentiment.polarity
    if sentiment > 0:
        return 'positive'
    elif sentiment < 0:
        return 'negative'
    else:
        return 'neutral'

# Add sentiment
PreProcessed_Data['sentiment'] = PreProcessed_Data['clean_tweet'].apply(sentimentCheck)

<ipython-input-18-e58b5340ab7e>:2: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  PreProcessed_Data['sentiment'] = PreProcessed_Data['clean_tweet'].apply(sentimentCheck)

PreProcessed_Data.head()

	ids 	clean_tweet 	sentiment
462211 	2174633709 	would go bad eventually hey make fun b c rain 	negative
906600 	1695475403 	good morning hoping good one ya 	positive
953710 	1824695185 	makeup stuffs gon na good day 	positive
777548 	2322447234 	crying sleep really really really wanted go 	neutral
644707 	2236096768 	getting hungry 10 50am fair 	positive

PreProcessed_Data['sentiment'].value_counts()

neutral     30453
positive    28217
negative    21330
Name: sentiment, dtype: int64

import seaborn as sns
import matplotlib.pyplot as plt


counts = PreProcessed_Data['sentiment'].value_counts()


sns.set(style="whitegrid")
sns.set_context("talk")


ax = sns.barplot(x=counts.index, y=counts.values, palette="rocket")


ax.set_title("Sentiment Counts")
ax.set_xlabel("Sentiment")
ax.set_ylabel("Count")

plt.show()

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

max_features = 50000  # Increase the number of features
tokenizer = Tokenizer(num_words=max_features, split=' ')
tokenizer.fit_on_texts(PreProcessed_Data['clean_tweet'].values)
X = tokenizer.texts_to_sequences(PreProcessed_Data['clean_tweet'].values)
X = pad_sequences(X, maxlen=100)

import nltk
st = nltk.PorterStemmer()
def stemming_on_text(data):
  text = [st.stem(word) for word in data]
  return data
PreProcessed_Data['clean_tweet']=PreProcessed_Data['clean_tweet'].apply(lambda x: stemming_on_text(x))
PreProcessed_Data['clean_tweet']

<ipython-input-40-a9339844e2df>:6: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  PreProcessed_Data['clean_tweet']=PreProcessed_Data['clean_tweet'].apply(lambda x: stemming_on_text(x))

462211    would go bad eventually hey make fun b c rain
906600                  good morning hoping good one ya
953710                    makeup stuffs gon na good day
777548      crying sleep really really really wanted go
644707                      getting hungry 10 50am fair
                              ...                      
585164                homework budgets gon na difficult
344005                     argentina ready brazil goooo
125999                  going dubbo two months shithole
873726                        yuppers 10pm eastern time
122510                 played pet society ages slows pc
Name: clean_tweet, Length: 80000, dtype: object

lm = nltk.WordNetLemmatizer()
def lemmatizer_on_text(data):
  text = [lm.lemmatize(word) for word in data]
  return data
PreProcessed_Data['clean_tweet']=PreProcessed_Data['clean_tweet'].apply(lambda x: lemmatizer_on_text(x))
PreProcessed_Data['clean_tweet']

<ipython-input-41-0a0843388723>:5: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  PreProcessed_Data['clean_tweet']=PreProcessed_Data['clean_tweet'].apply(lambda x: lemmatizer_on_text(x))

462211    would go bad eventually hey make fun b c rain
906600                  good morning hoping good one ya
953710                    makeup stuffs gon na good day
777548      crying sleep really really really wanted go
644707                      getting hungry 10 50am fair
                              ...                      
585164                homework budgets gon na difficult
344005                     argentina ready brazil goooo
125999                  going dubbo two months shithole
873726                        yuppers 10pm eastern time
122510                 played pet society ages slows pc
Name: clean_tweet, Length: 80000, dtype: object

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D

embedding_dim = 256  # Increase the embedding dimension
lstm_units = 128  # Decrease LSTM units
vocab_size = max_features

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.4))
model.add(LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.

from sklearn.model_selection import train_test_split

one_hot_sentiment = pd.get_dummies(PreProcessed_Data['sentiment']).values

X_train, X_test, Y_train, Y_test = train_test_split(X, one_hot_sentiment, test_size=0.25, random_state=42)

print("Training set shapes - X: {}, Y: {}".format(X_train.shape, Y_train.shape))
print("Testing set shapes - X: {}, Y: {}".format(X_test.shape, Y_test.shape))

Training set shapes - X: (60000, 100), Y: (60000, 3)
Testing set shapes - X: (20000, 100), Y: (20000, 3)

batch_size = 64
epochs = 15

# Train the model
#history = model.fit(X_train[:2000], Y_train[:2000],
#                    epochs=epochs,
#                   batch_size=batch_size,
#                   verbose=2)

validation_size = 1500

X_validation = X_test[-validation_size:]
Y_validation = Y_test[-validation_size:]
X_testValidation = X_test[:-validation_size]
Y_testValidation = Y_test[:-validation_size]

# Get accuracy
score, acc = model.evaluate(X_testValidation, Y_testValidation, verbose=2, batch_size=batch_size)
print("Test score: %.2f" % score)
print("Test accuracy: %.2f" % acc)

290/290 - 10s - loss: 1.0987 - accuracy: 0.3255 - 10s/epoch - 35ms/step
Test score: 1.10
Test accuracy: 0.33

# Train the model with validation data
history = model.fit(X_train[:2000], Y_train[:2000],
                    validation_data=(X_validation, Y_validation),  # Add validation data
                    epochs=epochs,
                    batch_size=batch_size,
                    verbose=2)

Epoch 1/10
17/17 - 14s - loss: 1.0583 - accuracy: 0.4230 - val_loss: 1.0366 - val_accuracy: 0.4813 - 14s/epoch - 823ms/step
Epoch 2/10
17/17 - 9s - loss: 0.9887 - accuracy: 0.5140 - val_loss: 0.9907 - val_accuracy: 0.5007 - 9s/epoch - 516ms/step
Epoch 3/10
17/17 - 7s - loss: 0.9071 - accuracy: 0.5615 - val_loss: 0.9421 - val_accuracy: 0.5420 - 7s/epoch - 422ms/step
Epoch 4/10
17/17 - 8s - loss: 0.7416 - accuracy: 0.7135 - val_loss: 0.8031 - val_accuracy: 0.6467 - 8s/epoch - 479ms/step
Epoch 5/10
17/17 - 6s - loss: 0.4766 - accuracy: 0.8385 - val_loss: 0.6940 - val_accuracy: 0.7280 - 6s/epoch - 361ms/step
Epoch 6/10
17/17 - 13s - loss: 0.2999 - accuracy: 0.9115 - val_loss: 0.7092 - val_accuracy: 0.7433 - 13s/epoch - 789ms/step
Epoch 7/10
17/17 - 15s - loss: 0.2025 - accuracy: 0.9375 - val_loss: 0.7632 - val_accuracy: 0.7360 - 15s/epoch - 889ms/step
Epoch 8/10
17/17 - 8s - loss: 0.1379 - accuracy: 0.9600 - val_loss: 0.7027 - val_accuracy: 0.7593 - 8s/epoch - 464ms/step
Epoch 9/10
17/17 - 6s - loss: 0.1044 - accuracy: 0.9690 - val_loss: 0.8099 - val_accuracy: 0.7647 - 6s/epoch - 380ms/step
Epoch 10/10
17/17 - 7s - loss: 0.0820 - accuracy: 0.9810 - val_loss: 0.8290 - val_accuracy: 0.7693 - 7s/epoch - 418ms/step

# Plot training and validation accuracy
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training and validation loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Predict on the validation data
Y_pred = model.predict(X_validation)
# Convert one-hot encoded predictions back to labels
Y_pred_labels = np.argmax(Y_pred, axis=1)
Y_true_labels = np.argmax(Y_validation, axis=1)
from sklearn.metrics import confusion_matrix

# Your code here that uses confusion_matrix

# Create the confusion matrix
confusion_mat = confusion_matrix(Y_true_labels, Y_pred_labels)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

47/47 [==============================] - 1s 23ms/step

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, SpatialDropout1D
from tensorflow.keras.optimizers import Adam

embedding_dim = 128
lstm_units = 196
vocab_size = 2000

model1 = Sequential()
model1.add(Embedding(vocab_size, embedding_dim, input_length=X.shape[1]))
model1.add(SpatialDropout1D(0.4))
model1.add(Bidirectional(LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2)))
model1.add(Dense(3, activation='softmax'))

# Compile the model with the Adam optimizer
optimizer = Adam(learning_rate=1e-3)
model1.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

print(model1.summary())

WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.

Model: "sequential_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_3 (Embedding)     (None, 80, 128)           256000    
                                                                 
 spatial_dropout1d_3 (Spati  (None, 80, 128)           0         
 alDropout1D)                                                    
                                                                 
 bidirectional_1 (Bidirecti  (None, 392)               509600    
 onal)                                                           
                                                                 
 dense_3 (Dense)             (None, 3)                 1179      
                                                                 
=================================================================
Total params: 766779 (2.93 MB)
Trainable params: 766779 (2.93 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None

batch_size = 120
epochs = 10
validation_size = 1500

X_validation = X_test[-validation_size:]
Y_validation = Y_test[-validation_size:]
X_testValidation = X_test[:-validation_size]
Y_testValidation = Y_test[:-validation_size]

# Get accuracy
score, acc = model1.evaluate(X_testValidation, Y_testValidation, verbose=2, batch_size=batch_size)
print("Test score: %.2f" % score)
print("Test accuracy: %.2f" % acc)

155/155 - 8s - loss: 1.0966 - accuracy: 0.3381 - 8s/epoch - 51ms/step
Test score: 1.10
Test accuracy: 0.34

# Train the model with validation data
history1 = model1.fit(X_train[:2000], Y_train[:2000],
                    validation_data=(X_validation, Y_validation),  # Add validation data
                    epochs=epochs,
                    batch_size=batch_size,
                    verbose=2)

Epoch 1/10
17/17 - 22s - loss: 1.0698 - accuracy: 0.4155 - val_loss: 1.0513 - val_accuracy: 0.4313 - 22s/epoch - 1s/step
Epoch 2/10
17/17 - 13s - loss: 0.9909 - accuracy: 0.5035 - val_loss: 0.9982 - val_accuracy: 0.4980 - 13s/epoch - 791ms/step
Epoch 3/10
17/17 - 14s - loss: 0.9142 - accuracy: 0.5450 - val_loss: 0.9352 - val_accuracy: 0.5373 - 14s/epoch - 794ms/step
Epoch 4/10
17/17 - 13s - loss: 0.7397 - accuracy: 0.7135 - val_loss: 0.8492 - val_accuracy: 0.5860 - 13s/epoch - 772ms/step
Epoch 5/10
17/17 - 14s - loss: 0.5018 - accuracy: 0.8290 - val_loss: 0.7157 - val_accuracy: 0.7287 - 14s/epoch - 813ms/step
Epoch 6/10
17/17 - 14s - loss: 0.3145 - accuracy: 0.9000 - val_loss: 0.6679 - val_accuracy: 0.7507 - 14s/epoch - 797ms/step
Epoch 7/10
17/17 - 13s - loss: 0.2264 - accuracy: 0.9280 - val_loss: 0.6864 - val_accuracy: 0.7653 - 13s/epoch - 787ms/step
Epoch 8/10
17/17 - 13s - loss: 0.1570 - accuracy: 0.9540 - val_loss: 0.7742 - val_accuracy: 0.7540 - 13s/epoch - 780ms/step
Epoch 9/10
17/17 - 13s - loss: 0.1106 - accuracy: 0.9690 - val_loss: 0.7916 - val_accuracy: 0.7660 - 13s/epoch - 792ms/step
Epoch 10/10
17/17 - 13s - loss: 0.0964 - accuracy: 0.9745 - val_loss: 0.7835 - val_accuracy: 0.7667 - 13s/epoch - 762ms/step

# Plot training and validation accuracy
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training and validation loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

